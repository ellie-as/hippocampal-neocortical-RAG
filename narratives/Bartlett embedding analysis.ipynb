{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73d01e6-8dd6-4d17-93ad-917ed84863ad",
   "metadata": {},
   "source": [
    "### Bartlett embedding analysis\n",
    "\n",
    "* Embed 'background data'\n",
    "* Embed recalled stories for each model\n",
    "* Project into 2D\n",
    "* Do the recalled stories get closer to the background distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421adb4-498d-4d28-9328-ba54cd7bd321",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2077f-57d7-40fd-823f-1dc4107162a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import string\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Path to the directory containing pickle files\n",
    "directory_path = '.' #'bartlett_data'\n",
    "\n",
    "# Load embedding model for analysing text\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf86038-f7ae-47ab-82a5-652bd5276086",
   "metadata": {},
   "source": [
    "#### Analyse embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507755a-cc31-42f4-aaa9-16f10f25796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bartlett story\n",
    "bartlett = \"\"\"One night two young men from Egulac went down to the river to hunt seals and while they were there it became foggy and calm. Then they heard war-cries, and they thought: \"Maybe this is a war-party\". They escaped to the shore, and hid behind a log. Now canoes came up, and they heard the noise of paddles, and saw one canoe coming up to them. There were five men in the canoe, and they said:\n",
    "\"What do you think? We wish to take you along. We are going up the river to make war on the people.\"\n",
    "One of the young men said,\"I have no arrows.\"\n",
    "\"Arrows are in the canoe,\" they said.\n",
    "\"I will not go along. I might be killed. My relatives do not know where I have gone. But you,\" he said, turning to the other, \"may go with them.\"\n",
    "So one of the young men went, but the other returned home.\n",
    "And the warriors went on up the river to a town on the other side of Kalama. The people came down to the water and they began to fight, and many were killed. But presently the young man heard one of the warriors say, \"Quick, let us go home: that man has been hit.\" Now he thought: \"Oh, they are ghosts.\" He did not feel sick, but they said he had been shot.\n",
    "So the canoes went back to Egulac and the young man went ashore to his house and made a fire. And he told everybody and said: \"Behold I accompanied the ghosts, and we went to fight. Many of our fellows were killed, and many of those who attacked us were killed. They said I was hit, and I did not feel sick.\"\n",
    "He told it all, and then he became quiet. When the sun rose he fell down. Something black came out of his mouth. His face became contorted. The people jumped up and cried.\n",
    "He was dead.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3de410-8397-469a-8e0e-a725742029cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "# Function to load data from a pickle file\n",
    "def load_pickle_data(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "# Read and combine data from all pickle files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.pkl'):  # Ensures that we are reading only pickle files\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        data = load_pickle_data(file_path)\n",
    "        print(filename)\n",
    "        print(data.keys())\n",
    "        print([k for k, v in data.items() if len(v)])\n",
    "\n",
    "        for category in ['Universe', 'Politics', 'Health', 'Sport', 'Technology', 'Nature']:\n",
    "            ckpts = sorted(data[category], key=lambda name: int(name.split('-')[-1]))\n",
    "            epoch_map = {ck: i+1 for i, ck in enumerate(ckpts)}\n",
    "            for ckpt in data[category]:\n",
    "                for temp in [0, 0.5, 1, 1.5]:\n",
    "                    # Extend the list of strings for this category and temperature\n",
    "                    if type(data[category][ckpt][temp]) == str:\n",
    "                        records.append({\n",
    "                            'topic': category,\n",
    "                            'epoch': epoch_map[ckpt],\n",
    "                            'temp': temp,\n",
    "                            'text': data[category][ckpt][temp]\n",
    "                        })\n",
    "                    else:\n",
    "                        for story in data[category][ckpt][temp]:\n",
    "                            records.append({\n",
    "                                'topic': category,\n",
    "                                'epoch': epoch_map[ckpt],\n",
    "                                'temp': temp,\n",
    "                                'text': story\n",
    "                            })\n",
    "\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d7f0d-72b7-4be5-a68f-c9848bb28e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[(df['topic'] == 'Politics') & (df['epoch'] == 5) & (df['temp'] == 0)]['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb08e0a-d85c-4df1-aed8-9a77cf3d463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['topic'] == 'Universe'][df['temp'] == 0.5][df['epoch'] == 5]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df64522-7634-4153-8a73-9348a25eb831",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('tarekziade/wikipedia-topics')\n",
    "wiki_df = dataset['train'].to_pandas()\n",
    "\n",
    "def get_texts_by_category(category, dataframe):\n",
    "    # Filter the DataFrame for rows where the category list contains the specified category\n",
    "    # Remove articles about people (these tend to have many categories applied that reflect the content less)\n",
    "    filtered_df = dataframe[~dataframe['categories'].apply(lambda x: 'People' in x)]\n",
    "    filtered_df = dataframe[dataframe['categories'].apply(lambda x: category in x)]\n",
    "    return filtered_df['text'].sample(frac=1).tolist()\n",
    "\n",
    "universe_txts = [i[:len(bartlett)] for i in get_texts_by_category('Universe', wiki_df)][0:1000]\n",
    "politics_txts = [i[:len(bartlett)] for i in get_texts_by_category('Politics', wiki_df)][0:1000]\n",
    "health_txts = [i[:len(bartlett)] for i in get_texts_by_category('Health', wiki_df)][0:1000]\n",
    "sport_txts = [i[:len(bartlett)] for i in get_texts_by_category('Sports', wiki_df)][0:1000]\n",
    "tech_txts = [i[:len(bartlett)] for i in get_texts_by_category('Technology', wiki_df)][0:1000]\n",
    "nature_txts = [i[:len(bartlett)] for i in get_texts_by_category('Nature', wiki_df)][0:1000]\n",
    "    \n",
    "temp = 0.5\n",
    "universe_stories = df[(df['topic'] == 'Universe') & (df['temp'] == temp) & (df['epoch'] == 5)]['text'].tolist()\n",
    "politics_stories = df[(df['topic'] == 'Politics') & (df['temp'] == temp) & (df['epoch'] == 5)]['text'].tolist()\n",
    "health_stories = df[(df['topic'] == 'Health') & (df['temp'] == temp) & (df['epoch'] == 5)]['text'].tolist()\n",
    "sport_stories =  df[(df['topic'] == 'Sport') & (df['temp'] == temp) & (df['epoch'] == 5)]['text'].tolist()\n",
    "tech_stories = df[(df['topic'] == 'Technology') & (df['temp'] == temp) & (df['epoch'] == 5)]['text'].tolist()\n",
    "nature_stories = df[(df['topic'] == 'Nature') & (df['temp'] == temp) & (df['epoch'] == 5)]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315cb25-24c7-40f2-9dda-bceccdd5515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts):\n",
    "    texts = [t[0:800] for t in texts]\n",
    "    return model.encode(texts)\n",
    "\n",
    "def calculate_mean_embeddings(*embedding_lists):\n",
    "    means = [np.mean(embeddings, axis=0) for embeddings in embedding_lists]\n",
    "    return np.array(means)\n",
    "   \n",
    "universe_embeddings = np.array([embed_texts([txt]) for txt in universe_txts])\n",
    "politics_embeddings = np.array([embed_texts([txt]) for txt in politics_txts])\n",
    "health_embeddings = np.array([embed_texts([txt]) for txt in health_txts])\n",
    "sport_embeddings = np.array([embed_texts([txt]) for txt in sport_txts])\n",
    "tech_embeddings = np.array([embed_texts([txt]) for txt in tech_txts])\n",
    "nature_embeddings = np.array([embed_texts([txt]) for txt in nature_txts])\n",
    "\n",
    "universe_story_embeddings = np.array([embed_texts([txt]) for txt in universe_stories])\n",
    "politics_story_embeddings = np.array([embed_texts([txt]) for txt in politics_stories])\n",
    "health_story_embeddings = np.array([embed_texts([txt]) for txt in health_stories])\n",
    "sport_story_embeddings = np.array([embed_texts([txt]) for txt in sport_stories])\n",
    "tech_story_embeddings = np.array([embed_texts([txt]) for txt in tech_stories])\n",
    "nature_story_embeddings = np.array([embed_texts([txt]) for txt in nature_stories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552b0f3-afb1-436b-aac5-e98ee4d809c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bartlett_text = bartlett\n",
    "bartlett_embedding = embed_texts([bartlett])\n",
    "bartlett_embedding = bartlett_embedding.reshape(1, -1)  # Ensure it has the right shape\n",
    "\n",
    "# Apply PCA to all embeddings\n",
    "all_embeddings = np.concatenate([universe_embeddings, politics_embeddings, sport_embeddings, tech_embeddings,  health_embeddings, nature_embeddings, \n",
    "                                 universe_story_embeddings, politics_story_embeddings, sport_story_embeddings, tech_story_embeddings, health_story_embeddings,  nature_story_embeddings,\n",
    "                                ])\n",
    "pca = PCA(n_components=2, random_state=1)\n",
    "reduced_embeddings = pca.fit_transform(all_embeddings.reshape(all_embeddings.shape[0], all_embeddings.shape[-1]))\n",
    "\n",
    "# Apply PCA to the Bartlett embedding using the already fitted PCA model\n",
    "reduced_bartlett_embedding = pca.transform(bartlett_embedding)\n",
    "\n",
    "# Calculate the reduced means after PCA for consistency in transformation\n",
    "mean_embeddings = calculate_mean_embeddings(universe_embeddings, politics_embeddings, sport_embeddings, tech_embeddings , health_embeddings, nature_embeddings,\n",
    "                                            universe_story_embeddings, politics_story_embeddings, sport_story_embeddings, tech_story_embeddings, health_story_embeddings, nature_story_embeddings\n",
    "                                           )\n",
    "reduced_means = pca.transform(mean_embeddings.reshape(mean_embeddings.shape[0], mean_embeddings.shape[-1]))\n",
    "\n",
    "# Labels and colors for each group\n",
    "labels = ['Universe data', 'Politics data', 'Sport data', 'Technology data', 'Health data', 'Nature data',\n",
    "          'Recalled (Universe)', 'Recalled (Politics)', 'Recalled (Sport)', 'Recalled (Technology)', 'Recalled (Health)', 'Recalled (Nature)'\n",
    "         ]\n",
    "\n",
    "colors = ['blue', 'green', 'purple', 'orange', 'cyan', 'red',\n",
    "          'blue', 'green', 'purple', 'orange', 'cyan', 'red'\n",
    "         ]\n",
    "\n",
    "base = ['#6a00a8', '#e16462', '#b12a90', '#0d0887', '#f0f921', '#fca636']\n",
    "colors = base + base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ad27b-5ba3-4523-852a-effa1df6aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "def plot_embeddings_with_inset(\n",
    "    embeddings,\n",
    "    reduced_means,\n",
    "    bartlett_embedding,\n",
    "    labels,\n",
    "    colors,\n",
    "    zoom_xlim=(-0.05, 0.05),\n",
    "    zoom_ylim=(0.08, 0.2),\n",
    "    inset_bbox=[0.5, 0.5, 0.47, 0.47],\n",
    "):\n",
    "    \"\"\"\n",
    "    embeddings:           (N×2) array of all points\n",
    "    reduced_means:       (M×2) array of your 12 mean‑points\n",
    "    bartlett_embedding:  (1×2) array for the single Bartlett point\n",
    "    labels:              list of length M\n",
    "    colors:              list of length M\n",
    "    zoom_xlim, zoom_ylim: the x/y limits for the inset\n",
    "    inset_bbox:          [x0,y0,width,height] in relative Axes coords\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5.8, 5))\n",
    "    \n",
    "    # 1) Main scatter: just the “data” groups (first 6)\n",
    "    group_sizes = [\n",
    "        len(universe_embeddings),\n",
    "        len(politics_embeddings),\n",
    "        len(sport_embeddings),\n",
    "        len(tech_embeddings),\n",
    "        len(health_embeddings),\n",
    "        len(nature_embeddings),\n",
    "        len(universe_story_embeddings),\n",
    "        len(politics_story_embeddings),\n",
    "        len(sport_story_embeddings),\n",
    "        len(tech_story_embeddings),\n",
    "        len(health_story_embeddings),\n",
    "        len(nature_story_embeddings),\n",
    "    ]\n",
    "    start = 0\n",
    "    for i, sz in enumerate(group_sizes):\n",
    "        end = start + sz\n",
    "        if i < 6:\n",
    "            ax.scatter(\n",
    "                embeddings[start:end, 0],\n",
    "                embeddings[start:end, 1],\n",
    "                color=colors[i],\n",
    "                alpha=0.35,\n",
    "                s=25,\n",
    "            )\n",
    "        start = end\n",
    "\n",
    "    # 2) Plot **all** the “means” >5 and arrows on the main Axes\n",
    "    for i, mean in enumerate(reduced_means):\n",
    "        if i > 5:\n",
    "            ax.scatter(\n",
    "                mean[0],\n",
    "                mean[1],\n",
    "                color=colors[i],\n",
    "                marker=\"o\",\n",
    "                s=25,\n",
    "                edgecolors=\"black\",\n",
    "                label=labels[i],\n",
    "            )\n",
    "            ax.arrow(\n",
    "                bartlett_embedding[0, 0],\n",
    "                bartlett_embedding[0, 1],\n",
    "                mean[0] - bartlett_embedding[0, 0],\n",
    "                mean[1] - bartlett_embedding[0, 1],\n",
    "                color=\"black\",\n",
    "                lw=0.5,\n",
    "                length_includes_head=True,\n",
    "                head_width=0.01,\n",
    "            )\n",
    "\n",
    "    # 3) Plot Bartlett on the main Axes\n",
    "    ax.scatter(\n",
    "        bartlett_embedding[0, 0],\n",
    "        bartlett_embedding[0, 1],\n",
    "        color=\"black\",\n",
    "        marker=\"o\",\n",
    "        s=25,\n",
    "        edgecolors=\"black\",\n",
    "        label=\"Original story\",\n",
    "    )\n",
    "    ax.legend(fontsize=10, ncol=1, loc=\"upper left\", markerscale=2)\n",
    "\n",
    "    # 4) Create the inset Axes and re‑plot the “zoomed” points there\n",
    "    axins = ax.inset_axes(inset_bbox, \n",
    "                          xlim=zoom_xlim, \n",
    "                          ylim=zoom_ylim, \n",
    "                          xticks=[], \n",
    "                          yticks=[])\n",
    "\n",
    "    # 4a) re‑plot the first six groups in the inset\n",
    "    start = 0\n",
    "    for i, sz in enumerate(group_sizes):\n",
    "        end = start + sz\n",
    "        if i < 6:\n",
    "            axins.scatter(\n",
    "                embeddings[start:end, 0],\n",
    "                embeddings[start:end, 1],\n",
    "                color=colors[i],\n",
    "                alpha=0.25,\n",
    "                s=130,\n",
    "            )\n",
    "        start = end\n",
    "\n",
    "    for i, mean in enumerate(reduced_means):\n",
    "        if i > 5:\n",
    "            axins.scatter(\n",
    "                mean[0],\n",
    "                mean[1],\n",
    "                color=colors[i],\n",
    "                marker=\"o\",\n",
    "                s=130,\n",
    "                edgecolors=\"black\",\n",
    "            )\n",
    "            axins.arrow(\n",
    "                bartlett_embedding[0, 0],\n",
    "                bartlett_embedding[0, 1],\n",
    "                mean[0] - bartlett_embedding[0, 0],\n",
    "                mean[1] - bartlett_embedding[0, 1],\n",
    "                color=\"black\",\n",
    "                lw=0.5,\n",
    "                length_includes_head=True,\n",
    "                head_width=0.015,\n",
    "            )\n",
    "    axins.scatter(\n",
    "        bartlett_embedding[0, 0],\n",
    "        bartlett_embedding[0, 1],\n",
    "        color=\"black\",\n",
    "        marker=\"o\",\n",
    "        s=130,\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    # 5) Draw the connecting box & “zoom‐lines”\n",
    "    ax.indicate_inset_zoom(axins, edgecolor=\"black\", linewidth=1)\n",
    "\n",
    "    # 6) Save & show\n",
    "    plt.savefig('plots/Recalled 2D with inset.png', bbox_inches='tight', dpi=300)\n",
    "    plt.xlim(-0.95, 0.6)\n",
    "    plt.ylim(-0.63, 0.8)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f77d2a0-469e-4c93-a1e5-d4bd61d9ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings_with_inset(\n",
    "    reduced_embeddings,\n",
    "    reduced_means,\n",
    "    reduced_bartlett_embedding,\n",
    "    labels,\n",
    "    colors,\n",
    "    zoom_xlim=(-0.2, 0.02),\n",
    "    zoom_ylim=(-0.02, 0.25),\n",
    "    inset_bbox=[0.02, 0.02, 0.27, 0.4],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e799dc-45dc-4f4e-b32f-63b171694dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean embedding of a list of embeddings\n",
    "def mean_embedding(embeddings):\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Embedding the bartlett story\n",
    "bartlett_embedding = embed_texts([bartlett])[0]  # Remove batch dimension here\n",
    "\n",
    "# Dictionary to store results\n",
    "category_results = {}\n",
    "\n",
    "# Categories and their respective texts and stories\n",
    "categories = {\n",
    "    'Universe': (universe_txts, universe_stories),\n",
    "    'Politics': (politics_txts, politics_stories),\n",
    "    'Health': (health_txts, health_stories),\n",
    "    'Sport': (sport_txts, sport_stories),\n",
    "    'Nature': (nature_txts, nature_stories),\n",
    "    'Technology': (tech_txts, tech_stories)\n",
    "}\n",
    "\n",
    "# Compute mean embeddings and distances for each category\n",
    "for category, (texts, stories) in categories.items():\n",
    "    # Embed category texts and stories\n",
    "    category_embeddings = np.array([embed_texts([txt])[0] for txt in texts])  # Remove batch dimension\n",
    "    story_embeddings = np.array([embed_texts([txt[:len(bartlett)]])[0] for txt in stories])  # Remove batch dimension\n",
    "    \n",
    "    # Compute mean embeddings\n",
    "    category_mean = mean_embedding(category_embeddings)\n",
    "    story_mean = mean_embedding(story_embeddings)\n",
    "    \n",
    "    # Calculate distances\n",
    "    distance_bartlett_category = cosine(bartlett_embedding, category_mean)\n",
    "    distance_story_category = cosine(story_mean, category_mean)\n",
    "    \n",
    "    # Store results in dictionary\n",
    "    category_results[category] = {\n",
    "        'distance_bartlett_category': distance_bartlett_category,\n",
    "        'distance_story_category': distance_story_category\n",
    "    }\n",
    "\n",
    "# Output the results dictionary\n",
    "print(category_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85832d08-7ce7-40a8-a997-85f595d1db65",
   "metadata": {},
   "source": [
    "#### Compare distances to category means of original and recalled stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4aaff-d2f2-4d2f-bc21-956e13db38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bartlett_embedding = embed_texts([bartlett])[0]  # (D,)\n",
    "\n",
    "categories = {\n",
    "    'Universe':   (universe_txts, universe_stories),\n",
    "    'Politics':   (politics_txts, politics_stories),\n",
    "    'Health':     (health_txts, health_stories),\n",
    "    'Sport':      (sport_txts, sport_stories),\n",
    "    'Nature':     (nature_txts, nature_stories),\n",
    "    'Technology': (tech_txts, tech_stories)\n",
    "}\n",
    "\n",
    "category_results = {}\n",
    "\n",
    "for category, (texts, stories) in categories.items():\n",
    "    # Embed category texts\n",
    "    category_embeddings = np.vstack([embed_texts([t])[0] for t in texts])  # (N_texts, D)\n",
    "    category_mean = category_embeddings.mean(axis=0)  # (D,)\n",
    "\n",
    "    # Embed recalled stories\n",
    "    story_embeddings = np.vstack([embed_texts([s[:len(bartlett)]])[0] for s in stories])  # (N_stories, D)\n",
    "\n",
    "    # Distances from Bartlett → category mean (just one number per category)\n",
    "    dist_b_all = [np.linalg.norm(bartlett_embedding - category_mean)]\n",
    "\n",
    "    # Distances from each recalled story → category mean\n",
    "    dist_r_all = [np.linalg.norm(story_emb - category_mean) for story_emb in story_embeddings]\n",
    "\n",
    "    # Aggregate mean±SEM\n",
    "    category_results[category] = {\n",
    "        'mean_b': np.mean(dist_b_all),\n",
    "        'std_b': sem(dist_b_all, ddof=1) if len(dist_b_all) > 1 else 0.0,\n",
    "        'mean_r': np.mean(dist_r_all),\n",
    "        'std_r': sem(dist_r_all, ddof=1) if len(dist_r_all) > 1 else 0.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64f91d-875d-4670-b08c-351d26d46fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(category_results)\n",
    "x = np.arange(len(cats))\n",
    "width = 0.35\n",
    "\n",
    "means_b = [category_results[c]['mean_b'] for c in cats]\n",
    "sems_b  = [category_results[c]['std_b']  for c in cats]\n",
    "means_r = [category_results[c]['mean_r'] for c in cats]\n",
    "sems_r  = [category_results[c]['std_r']  for c in cats]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,2))\n",
    "ax.bar(x - width/2, means_b, width, yerr=sems_b, capsize=5, alpha=0.6, label='Original', color=base[0])\n",
    "ax.bar(x + width/2, means_r, width, yerr=sems_r, capsize=5, alpha=0.6, label='Recalled', color=base[5])\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(cats)\n",
    "ax.set_ylabel('Distance')\n",
    "ax.set_ylim(0.8, 1.2)\n",
    "ax.legend(loc='upper center', ncols=2)\n",
    "fig.tight_layout()\n",
    "plt.savefig('plots/Recalled stories.png', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
